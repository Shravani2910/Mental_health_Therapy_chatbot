# -*- coding: utf-8 -*-
"""RAG_LLM_using_Langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I2wHs9rh9nZQnQDpfdGwZ_axRnzXOBOV
"""

!pip install -U langchain-community

!pip install langchain_groq

!pip install  langchain langchain_groq chromadb pypdf

import os

from chromadb import Client
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
# Import ChatGroq from groq_langchain
from langchain_groq import ChatGroq

from google.colab import drive
drive.mount('/content/drive')

db_path = "/content/chroma_db"

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    """Creates a vector database from PDF files."""
    pdf_dir = "/content/drive/MyDrive/pdf_docs"
    documents = []
    for fn in os.listdir(pdf_dir):
        if fn.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(pdf_dir, fn))
            documents.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = splitter.split_documents(documents)

    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory=db_path
    )
    print(" Vector database created and persisted to", db_path)
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_templates = """
You are a compassionate mental health chatbot.
First, give the sentiment of the user query, then respond thoughtfully.

Context:
{context}

User: {question}
Chatbot:
"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context','question'])
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# 1) Build or load your vector DB
# 1) Build or load your vector DB
if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    # Initialize HuggingFaceBgeEmbeddings before passing it to Chroma
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

# 2) Initialize your LLM & QA chain
llm = initialize_llm()
qa_chain = setup_qa_chain(vector_db, llm)

